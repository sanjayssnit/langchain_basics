{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOutLnHR8zmYygWTTHx3nV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjayssnit/langchain_basics/blob/main/langchain_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries"
      ],
      "metadata": {
        "id": "szUisawDNndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-groq langchain-community"
      ],
      "metadata": {
        "id": "x60ZXGBNOrT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get API Key from Colab Secrets\n",
        "This is applicable only if you are using Google Colab. In case you use Jupyter notebooks or use visual studio code, you can create a .env file and store your secrets."
      ],
      "metadata": {
        "id": "5fbeiHsGNt9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "LLAMA_MODEL = \"llama-3.3-70b-versatile\""
      ],
      "metadata": {
        "id": "3qvXRG3QbEko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat without memory\n",
        "A simple chat with LLM without using memory to store conversation history. The LLM will treat each question separately as it wont understand the previous conversations and the complete context."
      ],
      "metadata": {
        "id": "2t_RBDpaOD31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(api_key=api_key, model=\"llama-3.3-70b-versatile\")\n",
        "\n",
        "def simple_chat_without_memory(user_query):\n",
        "  response = llm.invoke(user_query)\n",
        "  return response.content"
      ],
      "metadata": {
        "id": "YUE5d0vCbP1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = simple_chat_without_memory(\"I want to learn AI\")\n",
        "print(response1)"
      ],
      "metadata": {
        "id": "joGstAmNCzW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = simple_chat_without_memory(\"Can you give me a learning plan?\")\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "0NvItu_HCyVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat with memory managed using internal List\n",
        "A simple chat with LLM with basic memory management to store conversation history. The LLM will read each question along with the previous conversation and answer based on the complete context."
      ],
      "metadata": {
        "id": "4905dKlJOWRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "conversation = []\n",
        "\n",
        "def simple_chat_with_memory(user_query):\n",
        "  conversation.append(HumanMessage(content=user_query))\n",
        "  response = llm.invoke(conversation)\n",
        "  conversation.append(AIMessage(content=response.content))\n",
        "  return response.content"
      ],
      "metadata": {
        "id": "ZC5r6wYZD9fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response1 = simple_chat_with_memory(\"I want to learn AI\")\n",
        "print(response1)"
      ],
      "metadata": {
        "id": "WnnE_-8QUafD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation)"
      ],
      "metadata": {
        "id": "abnNcH3uUlW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = simple_chat_with_memory(\"Can you give me a learning plan?\")\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "5vh5UFiwUwUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat prompt template and Chaining\n",
        "Langchain's chaining concept using Chat prompt template and an LLM invocation"
      ],
      "metadata": {
        "id": "V5h_M4kWOvqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate(messages=[\n",
        "    (\"system\", \"You are a helpful personal assistant\"),\n",
        "    (\"human\", \"{user_query}\")\n",
        "])\n",
        "\n",
        "chain = prompt | llm\n",
        "response_chain = chain.invoke({\"user_query\":\"I have decided to learn AI\"})\n",
        "print(response_chain)"
      ],
      "metadata": {
        "id": "bPPZ8EUsWKt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_chain = chain.invoke({\"user_query\":\"Can you give me a learning plan?\"})\n",
        "print(response_chain)"
      ],
      "metadata": {
        "id": "QpWrf5z_dBrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cahining with memory managed in prompt template"
      ],
      "metadata": {
        "id": "6RzUhvqIO_-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_memory = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful personal assistant\"),\n",
        "    (\"placeholder\", \"{chat_history}\"),\n",
        "    (\"human\", \"{user_query}\")\n",
        "])\n",
        "\n",
        "chain_with_memory = prompt_with_memory | llm"
      ],
      "metadata": {
        "id": "2vhtSX6mFm_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "store = {}\n",
        "# Function to get user's conversation history based on the user's session id\n",
        "def get_history(session_id: str):\n",
        "  if session_id not in store:\n",
        "    store[session_id] = ChatMessageHistory()\n",
        "  return store[session_id]"
      ],
      "metadata": {
        "id": "dcBGJ16hMuYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableWithMessageHistory\n",
        "\n",
        "# Runnable with message history enables us to run a specified chain and\n",
        "# gets the user's history using a callback function.\n",
        "\n",
        "chat_with_memory = RunnableWithMessageHistory(\n",
        "    runnable=chain_with_memory,\n",
        "    get_session_history=get_history,\n",
        "    input_messages_key=\"user_query\",\n",
        "    history_messages_key=\"chat_history\"\n",
        ")\n",
        "\n",
        "response = chat_with_memory.invoke({\"user_query\":\"I want to learn AI\"}, {\"configurable\": {\"session_id\": \"user1\"}})\n",
        "print(response)"
      ],
      "metadata": {
        "id": "5MauaDwuJqyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = chat_with_memory.invoke({\"user_query\":\"Can you give me a learning plan\"}, {\"configurable\": {\"session_id\": \"user1\"}})\n",
        "print(response2)"
      ],
      "metadata": {
        "id": "62OaOtOmQCW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question from a different user is get's conversation history only for that user.\n",
        "response3 = chat_with_memory.invoke({\"user_query\":\"Can you give me a learning plan\"}, {\"configurable\": {\"session_id\": \"user2\"}})\n",
        "print(response3)"
      ],
      "metadata": {
        "id": "rbsgiN_rQb4Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}